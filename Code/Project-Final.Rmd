---
title: "Extracting insights from FOMC statements"
subtitle: "Data 607 Final Projet"
author: Tyrannosaurus (Alexander Ng, Arun Reddy, Henry Otuadinma, Jagdish Chhabria)
date: "May 12, 2019"
output:
  html_document:
    toc: true
    toc_float: true
    highlight: tango
    Collapsed: false
    code_folding: show
    theme: sandstone
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
***
#### R Packages Used

This assignment was accomplished by utilizing these packages for both data analysis and visualizations.
```{r message=FALSE, warning=FALSE, paged.print=FALSE}

library(dplyr)
library(SentimentAnalysis)
library(lubridate)
library(ggplot2)
library(tidyr)
library(stringr)
library(rlang)
library(tidyverse)
library(tidytext)
library(xlsx)
library(RCurl)
library(XML)
library(kableExtra)
library(tm)
library(ngram)
library(wordcloud)
library(ggridges)
library(ggplot2)
library(gridExtra)
library(rcompanion)
library(ggcorrplot)
library(caret)
library(e1071)
library(R.utils)
library(DT)
library(lattice)
library(kernlab)
library(mlbench)
library(caretEnsemble)
library(nnet)
library(LiblineaR)
library(knitr)

```


# 1. The Purpose of Study

Central bank announcements of interest rate trajectories and monetary policy are among the most impactful events to global financial markets. This project uses data science to analyze Federal Reserve policy statements and seeks insight on their sentiment and relationship to financial market variables. By conducting this study, we identify some tangible results of interest but also lay out a basic foundation for a programme of future inquiry.

Our approach to analyzing the FOMC statement is through the lense of textual analysis. Two approaches are used in this project: Sentiment analysis is applied to the FOMC text corpus and used to construct a sentiment time series. That time series is then used as an explanatory variable to compare to real-word financial time series. Time series charting and inear regression are the main tools of that approach.

The second approach is via text classification. The textual analysis of FOMC statements is not exclusive to data science. Financial market practitioners pore over every word, gesture and media interview of the Federal Reserve chairman or chairwoman, governors and District presidents. Some financial news vendors like Bloomberg even publish text comparisons - showing the redlined differences between two FOMC statements. Our approach via text classification attempts to automate the classification of each FOMC statement by several attributes. The attributes include the hawkishness or dovishness of the statement, the FOMC’s opinion of economic growth or labor market health. The method used here is to manually label each FOMC statement and then to train a support vector machine algorithm to predict each attribute.

This paper is organized as follows: Section 2 gives background on the Federal Reserve, the FOMC and past research. Section 3 describes the sources of data: the FOMC statements, the manual labelling of statements, the financial time series. Section 4 performs exploratory data analysis on these data sources. Section 5 conducts two analysis: the first half addresses the machine learning training work to classify statements by 5 attributes. the second half conducts sentiment analysis. Section 6 discusses the results and Section 7 concludes this project.

# 2. Background


## 2.1 Federal Reserve

### 2.1.1 Background

The Federal Reserve System - commonly called "the Fed" - serves as the central bank of the United States. Congress passed the Federal Reserve Act in 1913, which President Woodrow Wilson supported and signed into law on December 23, 1913. Congress structured the Fed as a distinctly American version of a central bank: a "decentralized" central bank, with Reserve Banks and Branches in 12 Districts spread across the country and coordinated by a Board of Governors in Washington, D.C. Congress also gave the Fed System a mixture of public and private characteristics. The 12 Reserve Banks share many features with private-sector corporations, including boards of directors and stockholders (the member banks within their Districts). The Board of Governors, though, is an independent government agency, with oversight responsibilities for the Reserve Banks.

The Fed conducts monetary policy, supervises and regulates banking, serves as lender of last resort, maintains an effective and efficient payments system, and serves as banker for banks and the U.S. government. Conducting the nation's monetary policy is one of the most important - and often the most visible - functions of the Fed.

### 2.1.2 Monetary Policy
So, what is monetary policy? Simply put, it refers to the actions taken by the Fed to influence the supply of money and credit in order to foster price stability (i.e. control inflation) and maintain maximum sustainable employment.  These two objectives are called the "dual mandate".  This distinguishes the Fed from other central banks which typically have a single mandate to control inflation.   

The Fed's instrument for implementing monetary policy is the FOMC's target for the federal funds rate - the interest rate at which banks lend to each other overnight. By buying and selling U.S. government securities in the open market, the Fed influences the interest rate that banks charge each other. Movements in this rate and expectations about those changes influence all other interest rates and asset prices in the economy.

The Federal Reserve also issues the nation's currency (Federal Reserve notes) and manages the amount of funds the banking system holds as reserves. Currency and reserves make up what is called the monetary base.  However, because the vast majority of money in the US economy is in intangible form rather than physical notes, monetary policy focuses on interest rates instead of currency supply.

In the early days of the FOMC, controversy swirled around how to structure the vote. Should monetary policy be set by the 12 Reserve Banks or the Board of Governors? Or both? In 1935 Congress decided that the seven Governors would vote along with only five of the 12 presidents. The president of the New York Fed always votes - since the Open Market Trading Desk operates in that District - along with four presidents who rotate from among the groups shown below. In that way, voting members always come from different parts of the country.


## 2.2 The FOMC

As long as the U.S. economy is growing steadily and inflation is low, few people give much thought to the Federal Open Market Committee (FOMC), the group within the Federal Reserve System charged with setting monetary policy. Yet, when economic volatility makes the evening news, this Committee and its activities become much more prominent. Investors and workers, shoppers and savers all pay more attention to the FOMC's decisions and the wording of its announcements at the end of each meeting.

Why? Because the decisions made by the FOMC have a ripple effect throughout the economy. The FOMC is a key part of the Federal Reserve System, which serves as the central bank of the United States. Among the Fed's duties are managing the growth of the money supply, providing liquidity in times of crisis, and ensuring the integrity of the financial system. The FOMC's decisions to change the growth of the nation's money supply affect the availability of credit and the level of interest rates that businesses and consumers pay. Those changes in money supply and interest rates, in turn, influence the nation's economic growth and employment in the short run and the general level of prices in the long run.

### 2.2.1 FOMC Meetings
The FOMC meets regularly - typically every six to eight weeks - in Washington, D.C., although the Committee can and does meet more often by phone or videoconference if needed. The meetings are generally one-day or two-day events, with the two-day meetings providing more time to discuss a special topic. Around the table in the Federal Reserve Board's headquarters sit all 19 FOMC participants (seven Governors and 12 Reserve Bank presidents) as well as select staff and economists from the Board and the Reserve Banks. Because of the nature of the discussions, attendance is restricted. A Reserve Bank president, for instance, typically brings along only one staff member, usually his or her director of research.

The objective at each meeting is to set the Committee's target for the federal funds rate - the interest rate at which banks lend to each other overnight - at a level that will support the two key objectives of U.S. monetary policy: price stability and maximum sustainable economic growth. The meeting's agenda follows a structured and logical process that results in well-informed and thoroughly deliberated decisions on the future course of monetary policy.

### 2.2.2 Structure of a Typical Meeting

The meeting begins with a report from the manager of the System Open Market Account (SOMA) at the Federal Reserve Bank of New York, who is responsible for keeping the federal funds rate close to the target level set by the FOMC. The manager explains how well the Open Market Trading Desk has done in hitting the target level since the last FOMC meeting and discusses recent developments in the financial and foreign exchange markets. Up next is the Federal Reserve Board's director of the Division of Research and Statistics, along with the director of the Division of International Finance. They review the Board staff's outlook for the U.S. economy and foreign economies. This detailed forecast is circulated the week before the meeting to FOMC members in what is called the "Greenbook" - named for its green cover in the days when it was a printed document.

Then the meeting progresses to the first of two "go-rounds," which are the core of FOMC meetings. During the first go-round, all of the Fed Governors and Reserve Bank presidents discuss how they see economic and financial conditions. The Reserve Bank presidents speak about conditions in their Districts, as well as offering their views on national economic conditions. The data and information discussed vary by region and therefore spotlight a wide range of industries. For example, one would expect the review of regional conditions in the San Francisco District to lend insight into the tech sector of Silicon Valley. 

The policymakers have prepared for this go-round through weeks of information gathering. Before the FOMC meeting, each Reserve Bank prepares a "Summary of Commentary on Current Economic Conditions," which is published two weeks before each meeting in what most people call the "Beige Book," for the color of its cover when originally printed. One Federal Reserve Bank, designated on a rotating basis, publishes the overall summary of the 12 District reports. The Reserve Bank presidents have also gathered information by talking with executives in a variety of business sectors and through meetings with the Banks' boards of directors and advisory councils. 

This first go-round covers valuable information about economic activity throughout the country, measured in hard data and recent anecdotal information, as well as the analysis and interpretation conveyed by the policymakers sitting around the table. This is a key way in which each region of the U.S. has input into the making of national monetary policy. This portion of the meeting concludes with the FOMC Chair summarizing the discussion and providing the Chair's own view of the economy. At this point, the policy discussion begins with the Federal Reserve Board's director of the Division of Monetary Affairs, who outlines the Committee's various policy options. 

The outlook options could include no change, an increase, or a decrease in the federal funds rate target. Each option is described, along with a clear rationale, the pros and cons, and some alternatives for how the Committee could explain its decision in a public statement to be released that afternoon. Then, there is a second go-round. The Reserve Bank presidents and Governors each make the best case for the policy alternative they prefer, given current economic conditions and their personal outlook for the economy. They also comment on how they think the statement explaining the decision should be worded. One of the most important aspects of an FOMC meeting is that all voices matter. The analysis and viewpoints of each committee participant - whether a voting member or not - play an instrumental role in the FOMC's policy decisions.

At the end of this policy go-round, the Chair summarizes a proposal for action based on the Committee's discussion, as well as a proposed statement to explain the policy decision. The Fed Governors and presidents then get a chance to question or comment on the Chair's proposed approach. Once a motion for a decision is on the table, the Committee tries to come to a consensus through its deliberations. Although the final decision is most often one that all can support, there are times when some differences of opinion may remain, and voting members may dissent.  At the end of the policy discussion, all seven of the Fed Governors and the five voting Reserve Bank presidents cast a formal vote on the proposed decision and the wording of the statement.

### 2.2.3 Announcing the Policy Decision

After the vote has been taken, the FOMC publicly announces its policy decision at 2:15 p.m. The announcement includes the federal funds rate target, the statement explaining its actions, and the vote tally, including the names of the voters and the preferred action of those who dissented.

In addition, the FOMC releases its official minutes three weeks after each meeting. The minutes include a more complete explanation of the views expressed, which allows the public to get a better sense of the range of views within the FOMC and promotes awareness and understanding of how monetary policy is made. In recent years, the FOMC has improved communications with the public.   What's more, the FOMC now releases Committee participants' projections for the economy and inflation four times a year, which provides added insight into the policymakers' perspectives.  

### 2.2.4 Implementing Policy

Once the FOMC establishes a target for the federal funds rate, the Open Market Trading Desk at the Federal Reserve Bank of New York conducts daily open market operations - buying or selling U.S. government securities on the open market - as necessary to achieve the federal funds rate target. Open market operations affect the amount of money and credit available in the banking system, thereby affecting interest rates, which in turn affect the spending decisions of households and businesses and ultimately the overall performance of the U.S. economy.

### 2.2.5 Connecting To Our Project

This detailed description of the FOMC serves two purposes: (a) to describe the monetary policymaking activities of the FOMC (b) to identify the dataset which we will analyze.  We are going to focus exclusively on the FOMC policy statements released at 2:15pm ET.  Anecdotally, these policy statements have the greatest short term impact on financial markets and potential for surprise.  In the next section, we identify past research that examines the FOMC policy statements from a data science perspective.

## 2.3 Past Research

Our project is inspired and guided by past work in this field. There is a research literature and past work on FOMC statement analysis using data science methods. A related but distinct literature on the financial market impact of central bank communications is also relevant motivation. Moreover, the authors are aware of several financial institutions and companies engaged in the use of machine learning techniques to analyze central bank communications. We will touch on these in turn.

Cannon’s 2015 paper on FOMC sentiment analysis uses the transcripts instead of FOMC statements. He uses the financial dictionary of Loughran-McDonald to construct sentiment and defines it using a bag of words count method. The R package we use to derive sentiment calculates the same metric which is:

sentiment(Document)=PositiveWords−NegativeWordsPositiveWords+NegativeWords
However, he compares the sentiment index against real economic activity proxied by the Chicago Fed National Activity Index – not against financial market variables. A 2011 paper by Lucca and Trebbi analyzes the FOMC statements directly but measure hawkish and dovish tone not sentiment. In addition, their measurement technique is to use search engines (Google or Factiva) to generate a correlation of the word count hits in the search engine’s corpus between the words “hawk” or “dove” and each relevant word or N-gram from the policy statement. They call their approach a semantic orientation method to analyzing the statements. This measurement technique is not reproducible and computationally impractical. Another paper by Schmeling and Wagner (2019) analyzes sentiment in European Central Bank (ECB) policy statements, which are in English, using the Loughran-McDonald dictionary. They find that tone does seem to affect the risk premia of equities through a risk-based channel. I.e. Higher beta stocks respond more to ECB tone than lower beta stocks. They also find that corporate bond credit spreads between BBB and AAA rated bonds tighten when tone is positive. Lastly, Fuksa and Sornette (2012) analyze FOMC Beige Book, minutes and policy statements for sentiment. They find predictive power in the Beige Book which is released 3 weeks before the policy statement. Thus, analyzing Beige Book sentiment could predict FOMC policy actions.

A separate literature on the central bank impact on asset prices finds the FOMC meetings and statements are important. Cieslak, et.al. (2018) find that US and global stock returns are driven by the FOMC meeting cycle. Since 1994 the equity premium has been earned on even numbered weeks of the FOMC cycle. However, Brusa, et. al. (2017) find evidence that no other central banks have the same equity market impact as the Fed.


# 3. Data
## 3.1 FOMC Statements

### 3.1.1 Collate links to FOMC statements from January 2007 till date (May 2019)

This is one of the primary sources of data. The URL for each FOMC statement from  January 2007 onwards is collated from the following website (https://www.federalreserve.gov/monetarypolicy). While historical statements are available from 1994 onwards, the choice of starting point i.e. January 2007 was made to strike a balance between quantity and quality (adequate representation through different periods of the economic cycle such as recession (2009) and growth (2018).

There is a total of 102 statements published during this period.

```{r}
links<-c("https://www.federalreserve.gov/newsevents/pressreleases/monetary20190130a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20190320a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20190501a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20180131a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20180321a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20180502a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20180613a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20180801a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20180926a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20181108a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20181219a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20170201a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20170315a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20170503a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20170614a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20170726a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20170920a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20171101a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20171213a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20160127a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20160316a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20160427a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20160615a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20160727a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20160921a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20161102a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20161214a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20150128a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20150318a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20150429a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20150617a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20150729a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20150917a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20151028a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20151216a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20140129a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20140319a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20140430a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20140618a.htm", 
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20140730a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20140917a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20141029a.htm",  
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20141217a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20130130a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20130320a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20130501a.htm", 
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20130619a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20130731a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20130918a.htm",  
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20131030a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20131218a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20120125a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20120313a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20120425a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20120620a.htm", 
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20120801a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20120913a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20121024a.htm",  
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20121212a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20110126a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20110315a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20110427a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20110622a.htm", 
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20110809a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20110921a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20111102a.htm",  
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20111213a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20100127a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20100316a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20100428a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20100623a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20100810a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20100921a.htm",  
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20101103a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20101214a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20090128a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20090318a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20090429a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20090624a.htm", 
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20090812a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20090923a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20091104a.htm",  
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20091216a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20080122b.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20080130a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20080318a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20080430a.htm", 
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20080625a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20080805a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20080916a.htm",  
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20081008a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20081029a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20081216b.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20070131a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20070321a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20070509a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20070618a.htm", 
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20070807a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20070817b.htm",  
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20070918a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20071031a.htm",
         "https://www.federalreserve.gov/newsevents/pressreleases/monetary20071211a.htm"
)
length(links)
```

### 3.1.2 Data Staging - prepare metadata for data extraction and create a dataframe

```{r}
# Extract year of publication from the statement's release date, and create a data frame with date, year and URL. 
statement.dates<-NULL
year<-NULL
for(i in seq(from=1, to=length(links))) {
  statement.dates[i]<-(str_extract(links[i],"[[:digit:]]+"))
  year[i]<-substr(statement.dates[i],1,4)
}
reports<-data.frame(year,statement.dates, links)
# Convert factors to characters
reports %<>% mutate_if(is.factor, as.character)%>% arrange(statement.dates)
```

### 3.1.3 Data Extraction via web-scraping

```{r}
# Loop through the statement links and scrape the content from the Federal Reserve website.
# Discard irrelevant portions of the extracted content i.e. preliminary paragraphs and last paragraph.
statement.content<-NULL
statement.length<-NULL
for(i in seq(from=1, to=length(reports$links))) {
stm.url<-getURL(reports$links[i])
stm.tree<-htmlTreeParse(stm.url,useInternal=TRUE )
stm.tree.parse<-unlist(xpathApply(stm.tree, path="//p", fun=xmlValue))
n<-(which(!is.na(str_locate(stm.tree.parse, "release")))+1)[1]
l<-length(stm.tree.parse)-1
# Condense separate paragraphs into one element per statement date
reports$statement.content[i]<-paste(stm.tree.parse[n:l], collapse = "")
# Remove line breaks
reports$statement.content[i]<-gsub("\r?\n|\r"," ",reports$statement.content[i])
#reports$statement.content[i]<-gsub("\\.+\\;+\\,+","",reports$statement.content[i])
# Count number of characters per statement
reports$statement.length[i]<-nchar(reports$statement.content[i])
#reports$statement.length[i]<-wordcount(reports$statement.content[i], sep = " ", count.function = sum)
}
```


```{r}
# Create R data object
saveRDS(reports, file = "fomc_data.rds")
```

### 3.1.4 Data cleansing - correct a statement date

```{r}
# Correct the date for one statement, because the URL is not in sync with the actual date inside the statement content
reports$statement.dates[match(c("20070618"),reports$statement.dates)]<-"20070628"
```

## 3.2 Human Classification

We use specialist knowledge of financial markets to read and manually label all 102 FOMC statements from 2007 to May 2019.   Specifically, five attributes were reviewed and manually collected into a CSV file.  This file was then merged with the previous FOMC web scraped data to build a merged classification enriched dataset.  In this section, we demonstrate the data wrangling steps for to merge the 2 data sets.  Then we define and illustrate with examples from various statements each of the possible outcomes for each of the 5 attributes.  Providing transparency to the classification method is essential to understand the challenges even to human judgment of understanding "FedSpeak".  

### 3.2.1 Wrangling the Data into a Merged Dataset

First we load the FOMC statement data set into memory as a dataframe.

```{r}
d4<-readRDS(file = "fomc_data.rds")
dim(d4)
str(d4)
```

We explicitly override the Date column to be imported as string because we will join these two dataframes on this data.
In other words, we use a date column of the format "yyyymmdd" in string format as the common key for joining disparate datasets.

```{r}
classificationFile = "https://raw.githubusercontent.com/completegraph/DATA607FINAL/master/Code/Classification_FOMC_Statements.csv"
cls = read_csv(classificationFile , col_types = cols( Date = col_character() ) )
cls %>% rename( Economic.Growth = "Economic Growth", Employment.Growth = "Employment Growth", Medium.Term.Rate = "Medium Term Rate", Policy.Rate = "Policy Rate") -> cls
str(cls)
```
### 3.2.2 Merging FOMC data and Classification Data

```{r}
d4 %>% inner_join( cls , by = c("statement.dates" = "Date")) %>%
  mutate( date_mdy = mdy(Date2)) %>%
  select(Index, 
         year ,
         statement.dates, 
         links, 
         statement.content, 
         statement.length ,
         date_mdy,
         Economic.Growth,
         Employment.Growth,
         Inflation,
         Medium.Term.Rate,
         Policy.Rate ) -> mgData
str(mgData)
```

Let us view the sample data from the statements

```{r}
mgData %>% select( Index, date_mdy, Economic.Growth, Employment.Growth, Inflation, Medium.Term.Rate, Policy.Rate) %>% kable() %>% kable_styling(bootstrap_options = c("hover", "striped")) %>%
scroll_box(width = "90%", height = "300px")
```

### 3.2.3  Exporting the Merged Data Frame

We export the merged dataframe as a single RDS object for research use.

```{r}
rds_filename = "fomc_merged_data_v2.rds"
saveRDS(mgData, file = rds_filename)
```

### 3.2.4 Economic Growth

The attribute Economic.Growth is assigned one of 3 classification:  Up, Flat or Down.   It refers to the near term trend in economic growth since the last FOMC meeting or within the last quarter (whichever is mentioned).   Most statements give an explicit assessment of economic growth in the first 3 sentences.  An example of a UP classification is the July 29, 2015 statement (below coloring is mine):

>Information received since the Federal Open Market Committee met in June indicates that $\color{red}{\text{economic activity has been expanding moderately in recent months}}$. 
An example of a FLAT classification is the March 18, 2015 statement:

>Information received since ... January suggests that $\color{red}{\text{economic growth has moderated somewhat.}}$
An example of a DOWN classification is in the May 3, 2017 statement:

>Information received since ... March indicates that ... $\color{red}{\text{growth in economic activity slowed.}}$.
Rarely does the FOMC statement exclude an assessment of near-term economic growth trends in the US.

### 3.2.5 Employment Growth

The attribute Employment.Growth refers to the near-term trend of the labor market in the US.  We use the same classification values as for Economic.Growth.  If the labor market indicators are improving, we mark the indicator as UP.   This requires a decrease in the unemployment rate (if stated) and/or an increase in jobs creation.   These two key indicators broadly define the health of the labor market.  

An example of an UP classification is in the Feb 1, 2017 statement:

>the labor market has continued to strengthen ...
>Job gains remained solid and the unemployment rate
>stayed near its recent low. 
An example of a FLAT classification is the Dec 13, 2011 statement where the indicators are mixed:

>While indicators point to some improvement in overall labor market conditions, >the unemployment rate remains elevated.
An example of a DOWN classification is the April 29, 2009 statement where the labor market is discussed indirectly:

>Household spending has shown signs of stabilizing but remains constrained by >$\color{red}{\text{ongoing job losses}}$, lower housing wealth, and tight >credit. Weak sales prospects and difficulties in obtaining credit have led >businesses to cut back on inventories, fixed investment, and $\color{red}{\text{staffing.}}$
Sometimes the FOMC statement does not mention labor market conditions.  In this case, we assume information is irrelevant or not a concern and assign a FLAT classification.

### 3.2.6 Inflation

When measuring inflation, we refer to the realized price fluctuation of core PCE (where available) in the period since the last FOMC meeting.  Sometimes, this is not explicitly stated.   Then, we see the overall price movements (including food and energy) since the last meeting.   Where this is unstated, we rely on market driven indicators of medium term inflation risk as described by the statement.   We don't rely on shifts in the long term inflation expectations.   Of the various metrics from the FOMC statements, this indicator is the most challenging to classify due to the multiple dimensions of inflation.

An example of a UP classification comes from the April 27, 2011 meeting:

>$\color{red}{\text{Commodity prices have risen}}$ significantly since last summer, and concerns about global supplies of crude oil have contributed to a further $\color{red}{\text{increase in oil prices}}$ since the Committee met in March.  $\color{red}{\text{Inflation has picked up}}$ in recent months, but longer-term inflation expectations have remained stable and measures of underlying inflation are still subdued.
An example of a FLAT classification comes from the November 8, 2018 meeting.  Note that the FOMC views a 2 percent inflation rate as the natural rate of inflation, thus inflation near 2 percent is perceived as flat.   FLAT refers to either an absence of information or a rate near the natural rate.

>On a 12-month basis, $\color{red}{\text{both overall inflation and inflation for items other than food and energy remain near 2 percent}}$. Indicators of longer-term inflation expectations are little changed, on balance.
An example of a DOWN classification comes from the Jan 28, 2009 statement during the depths of the final crisis.

>In light of the $\color{red}{\text{declines in the prices}}$ of energy and other commodities in recent months and the prospects for considerable economic slack, the Committee expects that $\color{red}{\text{inflation pressures}}$ will remain subdued in coming quarters. Moreover, the Committee sees some risk that inflation could persist for a time below rates that best foster economic growth and price stability in the longer term.
### Medium Term Outlook

The FOMC tries to provide guidance of where it believe the 1-2 year outlook for the target fed funds rate will be positioned based on current information.   Medium Term Outlook attempts to measure this guidance:

An example of a HAWK classification comes from the March 15, 2017 statement:

>The Committee expects that economic conditions will evolve in a manner that will warrant $\color{red}{\text{gradual increases in the federal funds rate}}$; the federal funds rate is likely to remain, for some time, below levels that are expected to prevail in the longer run. However, the actual path of the federal funds rate will depend on the economic outlook as informed by incoming data.
 
 An example of a DOVE classification comes from the Sept 17, 2014 statement:

>the Committee today reaffirmed its view that a $\color{red}{\text{highly accommodative stance of monetary policy remains appropriate}}$. In determining how long to maintain the current 0 to 1/4 percent target range for the federal funds rate, the Committee will assess progress--both realized and expected--toward its objectives of maximum employment and 2 percent inflation. This assessment will take into account a wide range of information, including measures of labor market conditions, indicators of inflation pressures and inflation expectations, and readings on financial developments. The Committee continues to anticipate, based on its assessment of these factors, that it likely will be appropriate to $\color{red}{\text{maintain the current target range for the federal funds rate for a considerable time}}$ after the asset purchase program ends, especially if projected inflation continues to run below the Committee's 2 percent longer-run goal, and provided that longer-term inflation expectations remain well anchored.

### 3.2.7 Policy Rate

This last attribute is objective not subjective.   It identifies whether the FOMC decides to raise, keep unchanged or lower the federal funds target rate.   On that basis, the classification is assigned.

## 3.3 Financial TimeSeries

Our observation period for the FOMC data is 2007-2019. We wanted a period to cover both hawkish and dovish periods. The period should have enough observations to include the 2008 financial crisis and a full business cycle and both useful in statistical estimation for regression analysis.

The selection of our time series data from FRED followed specific criteria. We choose them because of several considerations. First, we need a public source of financial time series data. FRED meets that requirement. Second, we need relevant time series. The choices of the 3 financial time series below meet those requirements.

The US Treasury 10 Year yield is associated with the bellwether fixed income asset. It is possibly the most followed bond yield in the world. The breadth of its historical data is more than sufficient to cover the observation period of our study.

The Russell 1000 Index which we will used below is a liquid and large cross section of the most well-known and large capitalization US stocks. The particular time series is a total return index (dividends are assumed to be reinvested.)

The US Federal Reserve Funds target rates are the policy rates of the FOMC. At each FOMC meeting, these rates are changed. Prior to 2014, the FOMC published a single point estimate of that rate. After 2014, the FOMC decided to published a tight range with an upper and lower bound on the federal funds rate. This gives some latitude for the Open market operations desk to buy and sell Treasuries within this range.

# 4. Exploratory Analysis
## 4.1 Statements


### 4.1.1 Analyse FOMC statement word lengths and word frequency 

```{r}
# Compute total statement length per year by aggregating across individual statements
yearly.length<-reports%>% group_by(year) %>% summarize(words.per.year=sum(statement.length))
yearly.length
```
As can be seen, the total statement length was the highest for the year 2014. As expected, the count for 2019 is low because the year is still in progress and there have been only 3 meetings so far this year.


```{r}
# Graph the total statement length per year
ggplot(yearly.length, aes(x=yearly.length$year,y=yearly.length$words.per.year))+geom_bar(stat="identity",fill="darkblue", colour="black") + coord_flip()+xlab("Year")+ylab("Statement Length")
```




```{r}
#Verify word count for a sample word in a sample statement
sample<-reports%>%filter(reports$statement.dates=="20140319")
sample[,4]
str_count(sample, pattern="inflation")
```


### 4.1.2  Trend in Statement Length by year and Fed Chair
It seems that the FOMC statements became progressively verbose under Chairman Bernanke until they reached a peak in 2014 when Janet Yellen took over as the Fed Chair. This can be attributed to the fact that during 2014, there was a lot of discussion around when the Fed would end the quantitative easing measures that it had put in place to combat the recession that ensued from the financial crisis. There were 2 schools of thought - one which felt that the time was right for the Fed to start trimming its large balance sheet and the other that wanted to wait a bit longer to see more definite signs of growth before starting to reverse the quantitative easing measures. So the Fed tried to provide more transparency into their thinking which resulted in longer FOMC statments.

Since 2014, the statements have gotten shorter. The current chairman Jerome Powell took over in February 2018.

```{r}

# Graph the annual trend in statement length, annotated by Fed Chair
p<-ggplot(reports, aes(x=year,y=statement.length))+geom_point(stat="identity",color=statement.dates)+scale_fill_brewer(palette="Pastel1")+theme(legend.position="right")+xlab("Year") + ylab("Length of Statement")
p + ggplot2::annotate("text", x = 4,y = 5000, label = "Bernanke", family="serif", fontface="bold", colour="blue", size=4)+ggplot2::annotate("text", x=10, y=5500, label="Yellen", family="serif", fontface="bold", colour="darkred",size=4)+ggplot2::annotate("text", x=13, y=3600, label="Powell", family="serif", fontface="bold", colour="black",size=4)+ggplot2::annotate("segment", x = 0, xend = 8.1, y = 2700, yend = 6500, colour = "blue", size=1, arrow=arrow(ends="both"))+ggplot2::annotate("segment", x = 8.1, xend = 12.1, y = 6500, yend = 3200, colour = "darkred", size=1, arrow=arrow(ends="both"))+ggplot2::annotate("segment", x = 12.1, xend = 14, y = 3200, yend = 3200, colour = "black", size=1, arrow=arrow(ends="both"))
  
```


### 4.1.3  Adding custom words and names to the list of stop words

Remove proper nouns and irrelevant words from further analysis by adding them as custom words to the stop words lexicon

```{r}
# Add custom words to the stop words list to exclude proper nouns/names and words such as "committee" which would provide no meangingful insight into the statement's sentiment analysis
#print(stop_words)
words<-c("committee", "ben", "geithner", "bernanke", "timothy", "hoenig", "thomas", "donald", "kevin", "mishkin", "kroszner", "kohn", "charles", "frederic")
lexicon<-c("Custom")
my.stop_words<-data.frame(words, lexicon)
colnames(my.stop_words)<-c("word","lexicon")
new.stop_words <- rbind(my.stop_words, stop_words)
new.stop_words$word<-as.character(new.stop_words$word)
new.stop_words$lexicon<-as.character(new.stop_words$lexicon)
head(new.stop_words)
```

### 4.1.4  Cleanse data - remove irrelevant characters and calculate the frequency of the main words per statement date

```{r}
# Strip out punctuations, white space and custom stop words, and calculate the word frequency by statement date
report.words<-reports %>%mutate(date = statement.dates, year = year, text= statement.content) %>% unnest(text) %>% unnest_tokens(word, text) %>%mutate(word = stripWhitespace(gsub("[^A-Za-z ]"," ",word))) %>% filter(word != "") %>% filter(word != " ") %>%anti_join(new.stop_words)%>% count(date, year, word, sort = TRUE)%>% mutate(frequency = n) %>% select(date, year, word, frequency)
```

### 4.1.5 Verify if the count is correct for a given combination of sample word and statement

```{r}
# Verify the count for the word "inflation" during the statements published in 2007 
report.words%>%filter(year=='2007', word=='inflation')
```


```{r }
# Rank most frequent words by year
f_text<-report.words%>% group_by(year,word) %>% summarize(total=sum(frequency))%>%arrange(year,desc(total),word)%>% mutate(rank=row_number())%>%ungroup() %>% arrange(rank,year)
# Select the top 10 ranked words per year
topWords <- f_text %>% filter(rank<11)%>%arrange(year,rank)
print(topWords)
```

### 4.1.6  Graph the most frequent words per year

```{r fig.width = 15, fig.asp = 0.95}
# Graph top 10 most frequent words by year
gg <- ggplot(head(topWords, 130), aes(y=total,x=reorder(word,rank))) + geom_col(fill="#27408b") +
  facet_wrap(~year,scales="free", ncol=3)+ coord_flip()+theme_ridges(font_size=11) + 
  labs(x="",y="",title="Most Frequent Words in FOMC Statements grouped by years (2007 - 2019)")
gg
```

### 4.1.7 Conclusion

As can be seen from the above analysis, the type of words that show up in the top 10 list are largely the same. This is because in almost all cases, the FOMC statements start by making a reference to the previous statement and refer to the common economic parameters that the committee tracks. So there is large amount of consistency in how the statements are worded and the type of terms they employ. There is no surprise in the most frequently used words in these statements. In fact, one could argue that it is the differential i.e. the new words which are likely to be the least frequently words in the statements that provide the real information needed for sentiment analysis.

On account of this, we do not pursue this path further, and change track to other approaches to do our analysis.

## 4.2 Human Classification

### 4.2.1 Exploratory Data Analysis:  Human Classification

We inspect the categorical data to check for distribution and  covariation patterns.


```{r}
mgData<-readRDS(file = "fomc_merged_data_v2.rds")
```

### 4.2.2 Frequency Distributions of Each Attribute

```{r}
gEcon <- ggplot(data=mgData, aes(x=Economic.Growth, fill=Economic.Growth)) + 
  geom_bar() + theme(legend.position = "none")
gEmp  <- ggplot(data=mgData, aes(x=Employment.Growth, fill=Employment.Growth)) + 
  geom_bar() +  theme(legend.position = "none")
gInf  <- ggplot(data=mgData, aes(x=Inflation, fill=Inflation)) + 
  geom_bar() + theme(legend.position = "none")
gRate <- ggplot(data=mgData, aes(x=Medium.Term.Rate, fill=Medium.Term.Rate)) + 
  geom_bar() + theme(legend.position = "none")
gPolicy <- ggplot(data=mgData, aes(x=Policy.Rate, fill=Policy.Rate)) + 
  geom_bar() + theme(legend.position = "none")
grid.arrange(gEcon, gEmp, gInf, gRate, gPolicy, ncol=3, nrow=2 )
```

Inspecting the above charts, we infer some tendencies and align them to our understanding of the markets.

* Economic.Growth is Up over 60% of the time.  This is consistent with the US economy have a positive growth rate over the last 200 years.  Since the long term trend is positive growth, the histogram is not a surprise.

* Employment.Growth is Up over 52% of the time.  This is likewise consistent with US economy having positive employment and economic growth.

* Inflation is down or flat for 80% of the time.  This is not consistent with the long term trend.  However, in the 2007-2019 period, inflation has been less than the long term trend. 

* Dove is 80% of the time.   This is inconsistent with long term trend where Dove and Hawk are more balanced. It is consistent with the 2008-2015 period being part of a long recovery cycle.  

* Policy rate is flat over 80% of the time.  This is consistent with the FOMC being a patient body that watches economic data and trends before acting.  Generally, the FOMC does not move rates at most meetings.  This is consistent with long term history.


### 4.2.3 Covariation of Attributes

Now we attempt to measure the degree of covariation between the categorical attributes.  Although we would use a correlation matrix if the data was continuous and normally distributed, the categorical data defies such an approach.  Luckily, we can introduce a new type of statistical measure called **Cramer's V** which measures the degree of association between two categorical variables.   Its value can vary from 0 (no association) to 1 (perfectly associated).   Like correlation, Cramer's V is symmetric in the variables $x$ and $y$.  Now we will calculate and display this measure using the *rcompanion* package and *ggcorrplot** package.  A reference to this statistic may be found here:  [https://en.wikipedia.org/w/index.php?title=Cram%C3%A9r%27s_V&oldid=882900387]


```{r}
mgData %>% select(Economic.Growth:Policy.Rate) -> catData  # categorical data
cv = matrix(rep(0,25), nrow=5, ncol=5)  # Allocate a 5x5 matrix of cramerV values initialized to 0.
for(idx in 1:5){
   for(jdx in 1:5){
       cv[idx,jdx] = cramerV(catData[,idx], catData[,jdx])
   }
}
rownames( cv ) = colnames(catData)
colnames( cv ) = colnames(catData)
ggcorrplot(cv, lab=TRUE, ggtheme = ggplot2::theme_classic(), colors=c("violet", "white", "lightgreen")) +
  ggtitle("CramerV Matrix", subtitle="Classification Attributes Comparison")
```

None of the CramerV values are high suggesting limited dependence between all variables.
The strongest association is between policy rate changes and medium term rate outlook at 0.44.   The surprising finding is that inflation is weakly associated with medium term rate outlook and policy rate changes.  One explanation suggests that inflation is not being strongly supervised by central bankers on the FOMC.  In this last business cycle, the key challenges have been financial crisis, significant unemployment and stagnant growth until the last 3 years.   Inflation has drifted sideways.


## 4.3 Financial TimeSeries

### 4.3.1 10-Year Treasury Constant Maturity Rate

Constant maturity is the theoretical value of a U.S. Treasury that is based on recent values of auctioned U.S. Treasuries. The value is obtained by the U.S. Treasury on a daily basis through interpolation of the Treasury yield curve which, in turn, is based on closing bid-yields of actively-traded Treasury securities. It is calculated using the daily yield curve of U.S. Treasury securities.
Constant maturity yields are often used by lenders to determine mortgage rates. The one-year constant maturity Treasury index is one of the most widely used, and is mainly used as a reference point for adjustable-rate mortgages (ARMs) whose rates are adjusted annually.
*Source: Investopedia*


```{r}

# Reading the data


DGS10<-read.csv("https://raw.githubusercontent.com/DataScienceAR/Cuny-Assignments/master/Data-607/Data-Sets/DGS10.csv",stringsAsFactors = FALSE)



# Data Cleanup

str(DGS10)
DGS10$DATE<- as_date(DGS10$DATE)
DGS10$DGS10<-as.numeric(DGS10$DGS10)

# Analysis of 10-Year Treasury Constant Maturity Rate

ggplot(data = DGS10)+
  aes(x=DATE,y=`DGS10`)+
  geom_line(size=.98,color="steelblue")+
  labs(x="Date",y="Percent",title="10 Year Constant Maturity Rate")+
  theme(panel.background = element_rect(fill = "white"))

```

The US Treasury 10 Year yield is associated with the bellwether fixed income asset. It is possibly the most followed bond yield in the world. The breadth of its historical data is more than sufficient to cover the observation period of our study.


### 4.3.2 Russell 3000® Total Market Index

The Russell 3000 Index is a market-capitalization-weighted equity index maintained by the FTSE Russell that provides exposure to the entire U.S. stock market. The index tracks the performance of the 3,000 largest U.S.-traded stocks which represent about 98% of all U.S incorporated equity securities.
The Russell 3000 Index serves as a building block for a broad range of financial products which include the large-cap Russell 1000 and the small-cap Russell 2000 index. The largest 1,000 stocks of the Russell 3000 constitute the Russell 1000, while the Russell 2000 is a subset of the smallest 2000 components. Unlike other funds, the Russell 3000 does not attempt to outperform a benchmark or take a defensive position when the markets appear overvalued; instead, it employs a fully passive strategy.
*Source: Investopedia*

```{r}

# Reading the data


RU3000TR<-read.csv("https://raw.githubusercontent.com/DataScienceAR/Cuny-Assignments/master/Data-607/Data-Sets/RU3000TR.csv",stringsAsFactors = FALSE)



# Data Cleanup

str(RU3000TR)
RU3000TR$DATE<- as_date(RU3000TR$DATE)
RU3000TR$RU3000TR<-as.numeric(RU3000TR$RU3000TR)

# Analysis of Russell 3000® Total Market Index

ggplot(data = RU3000TR)+
  aes(x=DATE,y=`RU3000TR`)+
  geom_line(size=.98,color="steelblue")+
  labs(x="Date",y="Percent",title="Russell 3000® Total Market Index")+
  theme(panel.background = element_rect(fill = "white"))

```


The Russell 3000 Index which we used above is a liquid and large cross section of the most well-known and large capitalization US stocks. The particular time series is a total return index (dividends are assumed to be reinvested.)


### 4.3.3 Russell 1000® Total Market Index

The Russell 1000 Index is an index of approximately 1,000 of the largest companies in the U.S. equity market. The Russell 1000 is a subset of the Russell 3000 Index. It represents the top companies by market capitalization. The Russell 1000 typically comprises approximately 90% of the total market capitalization of all listed U.S. stocks. It is considered a bellwether index for large-cap investing
The Russell 1000 is a much broader index than the often quoted Dow Jones Industrial Average and Standard & Poor's 500 Index, although all three are considered large cap stock benchmarks. (See also Investment Fundamentals: S&P 500 Index vs. Russell 1000 Index.) The Russell 1000 is managed by FTSE Russell. FTSE Russell also manages the Russell 3000 and Russell 2000 as well as numerous alternative indexes derived from each.
*Source: Investopedia*
```{r}

# Reading the data


RU1000TR<-read.csv("https://raw.githubusercontent.com/DataScienceAR/Cuny-Assignments/master/Data-607/Data-Sets/RU1000TR.csv",stringsAsFactors = FALSE)



# Data Cleanup

str(RU1000TR)
RU1000TR$DATE<- as_date(RU1000TR$DATE)
RU1000TR$RU1000TR<-as.numeric(RU1000TR$RU1000TR)

# Analysis of Russell 1000® Total Market Index

ggplot(data = RU1000TR)+
  aes(x=DATE,y=`RU1000TR`)+
  geom_line(size=.98,color="steelblue")+
  labs(x="Date",y="Percent",title="Russell 1000® Total Market Index")+
  theme(panel.background = element_rect(fill = "white"))

```


The Russell 1000 Index which we used above is a liquid and large cross section of the most well-known and large capitalization US stocks. The particular time series is a total return index (dividends are assumed to be reinvested.)


### 4.3.4 Federal Funds Target Range

The federal funds rate refers to the interest rate that banks charge other banks for lending them money from their reserve balances on an overnight basis. By law, banks must maintain a reserve equal to a certain percentage of their deposits in an account at a Federal Reserve bank. Any money in their reserve that exceeds the required level is available for lending to other banks that might have a shortfall.
Banks and other depository institutions are required to maintain non-interest-bearing accounts at Federal Reserve banks to ensure that they will have enough money to cover depositors' withdrawals and other obligations. How much money a bank must keep in its account is known as a reserve requirement and is based on a percentage of the bank's total deposits.

*Source: Investopedia*
```{r}

# Reading the data


FEDTARGET<-read.csv("https://raw.githubusercontent.com/DataScienceAR/Cuny-Assignments/master/Data-607/Data-Sets/FEDTARGET.csv",stringsAsFactors = FALSE)



# Data Cleanup

str(FEDTARGET)
FEDTARGET$DATE<- as.Date(strptime(FEDTARGET$DATE,format="%m/%d/%Y"),format="%Y-%m-%d")
FEDTARGET$Percent<-as.numeric(FEDTARGET$Percent)

# Analysis of Federal Funds Target Range

ggplot(data = FEDTARGET)+
  aes(x=DATE,y=`Percent`,color=Type)+
  geom_line(size=.98)+
  labs(x="Date",y="Percent",title="Federal Funds Target Range")+
  theme(panel.background = element_rect(fill = "white"))




```

The US Federal Reserve Funds target rates are the policy rates of the FOMC. At each FOMC meeting, these rates are changed. Prior to 2014, the FOMC published a single point estimate of that rate. After 2014, the FOMC decided to published a tight range with an upper and lower bound on the federal funds rate. This gives some latitude for the Open market operations desk to buy and sell Treasuries within this range.






  
# 5. Analysis

## 5.1 Text Classification


The manually labelled attributes of the FOMC statements are leveraged to validate the automated classification that we conduct below.   We hope this can serve a good basis for future research and that there is room for improvement.    Our analysis will be to run a predict training model on each of 5 attributes separately.    Although we will use the same code framework and model, the parameters used to tune and train the model will differ for each attribute.   Final results are summarized in a table after all the individual backtest results are presented.

One additional challenge of this data was that the categorical variables are not all binary.   Four of the attributes (variables) are ternary-valued.   For example, the Policy rate can be one of three values:  Raise, Flat, or Lower.   We overcome this challenge.  The caret framework is able to handle multiple-class attributes with no difficulty.
In the sentence: Data Preparation: Here, we prepare the data so that it can be reused for all the classifications without the need to repeat the leaning and preparation processes *gaain*

Data Preparation: Here, we prepare the data such that it can be reused for all the classifications without the need to repeat the cleaning and preparation processes again



```{r}
fomc_data <-readRDS(file = "fomc_merged_data_v2.rds")
```


```{r}
head(select(fomc_data, Index,year,statement.dates,statement.length,date_mdy,Employment.Growth,Economic.Growth,Inflation,Medium.Term.Rate,Policy.Rate))
```

##### Data preparation: First, randomising the rows so that the statements from different eras of the economic movements can be well represented

```{r}
set.seed(1234567)
fomc_Rand <- fomc_data[sample(nrow(fomc_data)),]
```


##### Preliminary data cleansing: convert the statements' textual contents to lower and remove `the federal open market committee` and `committee` as it is present in all the statements

```{r}
customStopWords <- c("the federal open market committee", "committee")
```


```{r}
fomc_dataX <- fomc_Rand %>% mutate(statement.content = tolower(statement.content))
fomc_dataX$statement.content <- str_replace_all(fomc_dataX$statement.content, customStopWords, "")
```


##### Data Preparation: Here, we prepare the data so that it can be reused for all the classifications without the need to repeat the leaning and preparation processes gaain

```{r}
# form a corpus
corpus <- VCorpus(VectorSource(fomc_dataX$statement.content))
# Remove Punctuation
corpus <- tm_map(corpus, content_transformer(removePunctuation))
# Remove numbers
corpus <- tm_map(corpus, removeNumbers)
# Convert to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
# Remove stop words
corpus <- tm_map(corpus, content_transformer(removeWords), stopwords("english"))
##Stemming
corpus <- tm_map(corpus, stemDocument)
# Remove Whitespace
corpus <- tm_map(corpus, stripWhitespace)
# Create Document Term Matrix
dtm <- DocumentTermMatrix(corpus)
# handle sparsity
corpusX <- removeSparseTerms(dtm, 0.95)
# convert to matrix
data_matrix <- as.matrix(corpusX)
```


##### From here, were are going to perform classifications based on `Medium.Term.Rate`, `Employment.Growth`, `Economic.Growth`, `Inflation`, and `Policy.Rate` variables


### 5.1.1 Medium.Term.Rate

##### Classification targetting the `Medium.Term.Rate` variable

```{r}
mRate <- data_matrix
# attach the 'medium.term.rate' column
mRate_matrix <- cbind(mRate, fomc_dataX$Medium.Term.Rate)
# rename it to 'tone'
colnames(mRate_matrix)[ncol(mRate_matrix)] <- "tone"
# convert to data frame
mRateData <- as.data.frame(mRate_matrix)
# convert 'tone' to lower case and make it a factor column as well
mRateData$tone <- as.factor(tolower(mRateData$tone))
```

##### Partition the data into training and test sets

```{r}
mRate_n <- nrow(mRateData)
mRateTrainVolume <- round(mRate_n * 0.80)
set.seed(314)
mRateTrainIndex <- sample(mRate_n, mRateTrainVolume)
mRateTrain <- mRateData[mRateTrainIndex,]
mRateTest <- mRateData[-mRateTrainIndex,]
```


```{r}
mRateModel <- train(tone ~., data = mRateTrain, method = 'svmLinear3')
```


```{r}
mRateResult <- predict(mRateModel, newdata = mRateTest)
```


```{r}
( mRateStats = confusionMatrix( mRateResult, mRateTest$tone))
```


### 5.1.2 Economic.Growth

##### Classification targeting the `Economic.Growth` variable

```{r}
econGrowth <- data_matrix
# attach the 'Economic.Growth' column
econG_matrix <- cbind(econGrowth, tolower(fomc_dataX$Economic.Growth))
# rename it to 'growth'
colnames(econG_matrix)[ncol(econG_matrix)] <- "egrowth"
# convert to data frame
econData <- as.data.frame(econG_matrix)
# convert 'growth' to a factor column as well
econData$egrowth <- as.factor(econData$egrowth)
```

##### Partition the data into training and test sets: note that the ratios here are different from the other models

```{r}
econ_n <- nrow(econData)
econTrainVolume <- round(econ_n * 0.58)
set.seed(314)
econTrainIndex <- sample(econ_n, econTrainVolume)
econTrain <- econData[econTrainIndex,]
econTest <- econData[-econTrainIndex,]
```


```{r}
econModel <- train(egrowth ~., data = econTrain, method = 'svmLinear3')
```


```{r}
econResult <- predict(econModel, newdata = econTest)
```


```{r}
(econStats = confusionMatrix( econResult, econTest$egrowth))
```


### 5.1.3 Inflation

##### Classification targeting the `Inflation` variable


```{r}
# Create Document Term Matrix
dtmI <- DocumentTermMatrix(corpus)
# handle sparsity
corpusI <- removeSparseTerms(dtm, 0.94)
# convert to matrix
data_matrixI <- as.matrix(corpusI)
inflation <- data_matrixI
# attach the 'Inflation' column
inflation_matrix <- cbind(inflation, tolower(fomc_dataX$Inflation))
# rename it to 'inflation'
colnames(inflation_matrix)[ncol(inflation_matrix)] <- "inflation"
# convert to data frame
inflationData <- as.data.frame(inflation_matrix)
# convert 'inflation' to a factor column
inflationData$inflation <- as.factor(inflationData$inflation)
```


##### Remove columns that will not contribute meaninfully to the model fitting

```{r}
infDataX <- inflationData[, -which(names(inflationData) %in% c("although", "william", "richard", "raphael", "randal", "san", "sarah","sandra", "togeth", "timothi","committe","dudley","esther"))]
 
```



```{r}
inf_n <- nrow(infDataX)
infTrainVolume <- round(inf_n * 0.70)
set.seed(314)
infTrainIndex <- sample(inf_n, infTrainVolume)
infTrain <- infDataX[infTrainIndex,]
infTest <- infDataX[-infTrainIndex,]
```


```{r}
inflationModel <- train(inflation ~., data = infTrain, method="svmLinear3")
```


```{r}
inflationResult <- predict(inflationModel, newdata = infTest)
```


```{r}
( infStats = confusionMatrix( inflationResult, infTest$inflation))
```


### 5.1.4 Employment.Growth

##### Classification targeting the `Employment.Growth` variable

```{r}
empGrowth <- data_matrix
# attach the 'Employment.Growth column
emp_matrix <- cbind(empGrowth, tolower(fomc_dataX$Employment.Growth))
# rename it to 'empGrowth'
colnames(emp_matrix)[ncol(emp_matrix)] <- "empGrowth"
# convert to data frame
empData <- as.data.frame(emp_matrix)
# convert 'empGrowth' to a factor column as well
empData$empGrowth <- as.factor(empData$empGrowth)
```


```{r}
emp_n <- nrow(empData)
empTrainVolume <- round(emp_n * 0.80)
set.seed(314)
empTrainIndex <- sample(emp_n, empTrainVolume)
empTrain <- empData[empTrainIndex,]
empTest <- empData[-empTrainIndex,]
```


```{r}
empModel <- train(empGrowth ~., data = empTrain, method = 'svmLinear3')
```


```{r}
empResult <- predict(empModel, newdata = empTest)
```


```{r}
( empStats = confusionMatrix( empResult, empTest$empGrowth))
```


### 5.1.5 Policy.Rate

##### Classification targeting the `Policy.Rate` variable

```{r}
plRate <- data_matrix
# attach the 'Policy.Rate' column
pl_matrix <- cbind(plRate, tolower(fomc_dataX$Policy.Rate))
# rename it to 'empGrowth'
colnames(pl_matrix)[ncol(pl_matrix)] <- "policy"
# convert to data frame
plData <- as.data.frame(pl_matrix)
# convert 'policy' to a factor column as well
plData$policy <- as.factor(plData$policy)
```



```{r}
pl_n <- nrow(plData)
plTrainVolume <- round(pl_n * 0.80)
set.seed(314)
plTrainIndex <- sample(pl_n, plTrainVolume)
plTrain <- plData[empTrainIndex,]
plTest <- plData[-empTrainIndex,]
```


```{r}
plModel <- train(policy ~., data = plTrain, method = 'svmLinear3')
```


```{r}
plResult <- predict(plModel, newdata = plTest)
```


```{r}
( plStats = confusionMatrix( plResult, plTest$policy))
```


### 5.1.6 Summary and Conclusion

Table of summary

```{r fig.width = 10, fig.asp = .80}


results <- tibble(variable = c("Medium.Term.Rate","Employment.Growth","Economic.Growth","Inflation","Policy.Rate"), modelling = c("80 : 20", "80 : 20", "58 : 42", "70 : 30", "80 : 20"), accuracy= c(100, 90, 79.1, 64.52, 95)) 

 kable(results,
      col.names = linebreak(c("Variable", "Modelling (Train : Test)", "Accuracy (%)"), align = "c")) %>%
  kable_styling("striped", full_width = T) %>%
  column_spec(1:3, bold = T, color = "#000") %>%
  row_spec(1:5, bold = T, color = "#000")
```


For some of the variables, extra fine-tuning of the data was not needed to achieve appreciable accuracy. But for the `Economic.Growth` variable, we needed to adjust the ratio of training set to test data set to `58:42` to achieve an accuracy of `79.10%`. For the `Inflation` variable we had to adjust sparsity to `0.94`, removed 13 unuseful columns, adjusted the ratio of training to test data sets to `70:30` before we could achieve an accuracy of `64.52%`

We can conclude that these values obtained, though not perfect, did go a long way to align with the human based scoring/classification of the economy trends based on the variables considered within the selected years. There are much room left for improvements and further analysis but we cannot go beyond this level right now as time will not permit us

## 5.2 Findings: Text Classification

Text classification of FOMC statements is not generally a research objective but we think it is worthwhile.   Classification addresses a potential need:  Can a machine correctly infer the opinion or direction of forward guidance or policy decisions in a structured text by the FOMC?   In this regard, the classification problem for the FOMC is isomorphic to the Ham-Spam classification of incoming emails by an email program.  However, the reader may object that FOMC statements are not so voluminous to require automated processing.   Our response is that FOMC statements are merely the first baby step in a much larger classification problem:  the public communications of all FOMC and Federal Reserve system members.   As previously explained, the FOMC members give speeches, publish articles, appear on TV interviews.   Moreover, FOMC meeting minutes are released several weeks after the policy statement is released.  These are much longer and required more effort to read and digest.  Also, the FOMC transcripts released several years after the meeting may run to over 100 pages each.  They contain word for word replay of the entire meeting (excluding private discussions).   Lastly, there are at least 16 relevant central banks around the world of interest.  Although the Fed is the world's more important central bank, the ECB, Bank of England, Bank of Japan, Bank of China, Bank of Australia, Bank of New Zealand, all produce communications.  In summary, no single person can read all central bank communications.   The ability to extract key messages from plain texts remains a valuable capability.

Our machine learning prediction backtest suggests that automated classification is feasible to detect limited features of a central bank communication.   Our algorithm succeeds at detecting medium term rate outlooks, employment growth and policy rate changes.  At these tasks, we have attained accuracy rates between 90-100 percent.   The most challenging attribute to understand is inflation (64.5%).  This is consistent with financial practitioner opinion.  Inflation is the most complex of these areas to quantify, control and manage.  That is because inflation has 4 distinct aspects:  realized inflation (price changes from past surveys), market based real yields of TIPS bonds and inflation swaps, and long term expectations of inflation, inflation measured with or without volatile sectors: food and energy.   Because the statements may treat some or all of these aspects we believe accuracy in understanding FOMC inflation views is hard.


## 5.3 Sentiment Analysis

### 5.3.1 Sentiment Analysis Used

Sentiment analysis is a research branch located at the heart of natural language processing (NLP), computational linguistics and text mining. It refers to any measures by which subjective information is extracted from textual documents. In other words, it extracts the polarity of the expressed opinion in a range spanning from positive to negative.
Current research in finance and the social sciences utilizes sentiment analysis to understand human decisions in response to textual materials. This immediately reveals manifold implications for practitioners, as well as those involved in the fields of finance research and the social sciences: researchers can use R to extract text components that are relevant for readers and test their hypotheses on this basis.

[source](https://cran.r-project.org/web/packages/SentimentAnalysis/vignettes/SentimentAnalysis.html#dictionary-generation)

### 5.3.2 Choice of Dictionary for text processing - Loughran-McDonoald
We used Lougran-McDonald Dictionary as our choice of Finance Dictionary for text processing. The Loughran-McDonald Master Dictionary was initially developed in conjunction with **our paper published in Journal of Finance**. The dictionary provides a means of determining which tokens (collections of characters) are actual words,which is important for consistency in word counts.

[source](https://cran.r-project.org/web/packages/SentimentAnalysis/vignettes/SentimentAnalysis.html#dictionary-generation)


### 5.3.3 Methods for Sentiment Analysis

As sentiment analysis is applied to a broad variety of domains and textual sources, research has devised various approaches to measuring sentiment. 
The citation of Pang and Lee 2008 should be added to the References section:  It will be:
Pang, Bo and Lillian Lee, "Opinion Mining and Sentiment Analysis", Foundations and Trends in Information Retrieval, Vol 2, Issue 1-2, January 2008, pp1-135.

In the process of performing sentiment analysis, one must convert the running text into a machine-readable format. This is achieved by executing a series of preprocessing operations. First, the text is tokenized into single words, followed by what are common preprocessing steps: stopword removal, stemming, removal of punctuation and conversion to lower-case. These operations are also conducted by default in SentimentAnalysis, but can be adapted to one’s personal needs.

### 5.3.4 Functionality of Sentiment Analysis
The process of Sentiment analysis tokenizes each document and finally converts the input into a document-term matrix. All of the previous operations are undertaken without manual specification. The analyzeSentiment() routine also accepts other input formats in case the user has already performed a preprocessing step or wants to implement a specific set of operations.


  ![](https://raw.githubusercontent.com/DataScienceAR/Cuny-Assignments/master/Data-607/Images/Formulae.jpg) 
  
                                 

```{r}
# Reading FOMC Data

fomcStatements <-readRDS(file = "fomc_merged_data_v2.rds") %>% select(statement.dates, statement.content)

```



```{r}

# Sentimental Analysis

fomcX <- fomcStatements %>%  mutate(date = statement.dates, year = as.numeric(str_extract(statement.dates,'\\d{4}')),text= statement.content)%>%   select(date, year, text)

```



```{r}

# Sentiment analysis with Loughran-Mcdonald dictionary

sentiment <- analyzeSentiment(fomcX$text, language = "english", aggregate = fomcX$year,
                              removeStopwords = TRUE, stemming = TRUE,
                              rules=list("SentimentLM"=list(ruleSentiment,
                                                            loadDictionaryLM())))

```



```{r}


# Summary of sentiment score

summary(sentiment)

# Table showing breakdown of Sentiments

table(convertToDirection(sentiment$SentimentLM))

# Line plot to visualize the evolvement of sentiment scores. This is especially helpful when studying a time series of sentiment scores.




plotSentiment(sentiment, xlab="Tone")

```


Conclusion: The Sentiment score is the lowest in the period from 2008 and 2009. The observation *correlates* to the recession during 2008 where market collapsed.


```{r}


Sentiment<-data.frame(fomcX$date,fomcX$year,sentiment$SentimentLM,convertToDirection(sentiment$SentimentLM))

names(Sentiment)<-(c("FOMC_Date","FOMC_Year","Sentiment_Score","Sentiment"))


# Structure before date type change

str(Sentiment)

# Change the date format to Ymd
Sentiment$FOMC_Date<- ymd(Sentiment$FOMC_Date)

# Change the Year format

Sentiment$FOMC_Year<- as.numeric(Sentiment$FOMC_Year)

# Structure after date type change

str(Sentiment)

# Distribution of Sentiment Score for period of 2007 to 2019


ggplot(Sentiment,aes(x=Sentiment_Score))+
  geom_histogram(binwidth =.0125,color="black",fill="lightblue")+
  labs(x="Setiment Score",y="Frequency",title="Sentiment Score Distribution from 2007 to 2019")+
   theme(panel.background = element_rect(fill = "white"))


```


The Overall sentiment frequency is towards negative than Positive score


```{r}
# Sentiment Score Trend

ggplot(data = Sentiment)+
  aes(x=FOMC_Date,y=Sentiment_Score)+
  geom_line(size=.98,color="firebrick")+
  labs(x="FOMC Date",y="Sentiment Score",title="Sentiment Score trend over the period of 2007 to 2019")+
   theme(panel.background = element_rect(fill = "white"))

```
The Sentiment score is the lowest in the period from 2008 and 2009. The observation corelates to the recession during 2008 where market collapsed.


```{r}

# Scatter plot of score vs Date

## Grouped

ggplot(Sentiment,aes(x=FOMC_Date,y=Sentiment_Score,color=Sentiment))+
  geom_point()+
  labs(x="FOMC Date",y="Sentiment Score",title="Sentiments spread over the period of 2007 to 2019")+
   theme(panel.background = element_rect(fill = "white"))

```

The chart shows the spread of negative sentiment throughtout the time period from 2008 to 2015, followed by the neutral and positive sentiment.

```{r}

# Exporting data frame to RDS
## Changing the Date format
Sentiment$FOMC_Date<-format(Sentiment$FOMC_Date, format = "%Y%m%d")
## Exporting to .RDS
saveRDS(Sentiment,"SentimentDF.rds")

```

## 5.4 Financial Impact of Sentiment

### 5.4.1 Sentiment and Equity Markets

In this section, we evaluate the relationship how our sentiment index compares to a broad US equity index (the Russell 1000 Index).   This section will examine the fluctuations of the sentiment compared to the equity market in two ways: through a visual analysis of the normalized levels of both variables and a linear regression of the time series data.
To accomplish this, we first merge 3 data sets aligned by the 102 FOMC meeting dates.  To calculate normalized versions of the variables, we calculate Z-scores of both variables over the sample period.   Lastly, we perform both analyses using the Z-score data.


```{r}
# First load all 3 files into data frames.
# ------------------------------------------------------
mgData<-readRDS(file = "fomc_merged_data_v2.rds")
sData <- readRDS( file = "../DATA/SentimentDF.rds")
file_fred_ru1000tr = "https://raw.githubusercontent.com/completegraph/DATA607FINAL/master/DATA/FRED_RU1000TR.csv"
ru1000tr = read_csv(file_fred_ru1000tr, 
                    col_types = cols(DATE=col_character(), 
                                     RU1000TR = col_double() ) )
# Generate a lubridate date column to join with the FOMC data.
# -----------------------------------------------------------------
ru1000tr %>% mutate( date_mdy = lubridate::ymd( DATE ) )-> ruData
 #z_ru_daily = (RU1000TR - mean(RU1000TR, na.rm=TRUE))/sd(RU1000TR, na.rm = TRUE )
#  Second, join the data:
#  Since this is a 2-way inner join, we start with the FOMC statement data
#  and join it to the sentiment data by date string (yyyymmdd)
# -------------------------------------------------------------------------
mgData %>% inner_join(sData, by = c( "statement.dates" = "FOMC_Date")) -> msData
#  Join the sentiment-FOMC data to the Russell 1000 Index data from FRED
#  Make sure to add a Z-score for each of the time series: sentiment and Rusell index values
#     Save the raw data and normalized data by FOMC data.
# ----------------------------------------------------------------------------------
msEQdata = msData %>% left_join(ruData, by = c("date_mdy" = "date_mdy") ) %>% 
                    select( date_mdy, Sentiment_Score, RU1000TR ) %>%
                    mutate( z_ru_fomc = (RU1000TR - mean(RU1000TR, na.rm = TRUE) ) / sd( RU1000TR, na.rm=TRUE ) ,
                            z_sentiment = ( Sentiment_Score - mean( Sentiment_Score, na.rm = TRUE) ) / 
                              sd( Sentiment_Score, na.rm=TRUE) )
```


### 5.4.2 Data Transformation:  Scale and Frequency Domain Issues

Let's inspect the data for accuracy and scaling issues.   Exploratory data analysis shows 3 issues.  

* Normalization to z-score format is needed to ensure that scale is not a problem.   Since the Russell Index level are expressed in the thousands, while the sentiment is on expressed in units of 0.01, scaling is essential along the y-dimension.  To solve the scale problem, we convert the entire sample to Z-score equivalent which bring both time series to the same order of magnitude and mean. 

* There is also a need to normalize in the frequency domain.   FOMC meetings occur 8 times per year so their sentiment levels and changes reflect nearly 2 months of news.   Russell equity index levels are collected on a daily basis in order to ensure completeness of the data collection.  The volatility of lower frequency data is much greater in absolute terms than volatility of higher frequency (daily) data.  To address this, we only calculate Z-scores of the Russell equity index levels observed **only** on the FOMC dates. 

* Lastly, Russell Index levels increases at a geometric rate (roughly).  Thus, values at the start of the sample period are smaller than values at the end of the period.   The residuals in a regression of such data show significant increase volatility over the sample period.   This is solved by apply a logarithmic transformation to Russell Index levels.   This change fixes the non-constant residual volatility and also improves the model fit from 36 to 39 percent adjusted R-squared roughly.


The following code produces the log-transformed z-scores of FOMC periodic equity values.

```{r eval=TRUE}
msEQdata %>% mutate( logEquity = log(RU1000TR) ) %>%
             mutate( z_logEquity = ( logEquity - mean(logEquity) )/ sd( logEquity ) ) -> msEQdata
  
msEQdata %>%  kable() %>% scroll_box(width="100%", height="200px")
```

### 5.4.3 Charting the Time Series Alternatives

In this section, we will show 3 time series charts illustrating the alternative considerations of regression modeling.

The first chart below shows the raw sentiment compared to raw Russell equity levels.  Scale issues are obvious since the sentiment values are compressed to the appearance of a slightly fuzzy flat line.  The chart below shows scaling is essential.

```{r}
ggplot() + 
  geom_line(data=msEQdata, aes(x=date_mdy, y=Sentiment_Score) , color = "red" ) +
  geom_line(data=msEQdata, aes(x=date_mdy, y=RU1000TR), color="green") +
  ggtitle("Sentiment vs. Russell 1000 Equity Level", subtitle="Not usable without fixes")
  
```

The second chart shows the use of scaled sentiment versus scaled Russell equity levels.  Scale issues are remain because the right hand side (the more recent years) shows higher variation than the left hand side (earliest years).

```{r}
ggplot() + 
  geom_line(data=msEQdata, aes(x=date_mdy, y=z_sentiment) , color = "red" ) +
  geom_line(data=msEQdata, aes(x=date_mdy, y=z_ru_fomc), color="green") +
  ggtitle("Scaled Sentiment vs. Scaled Equity Index", subtitle = "Nearly There...")
```

Finally, the third chart shows the variables we will use in the regression analysis.

```{r, eval=TRUE}
ggplot() + 
  geom_line(data=msEQdata, aes(x=date_mdy, y=z_sentiment) , color = "red" ) +
  geom_line(data=msEQdata, aes(x=date_mdy, y=z_logEquity), color="green") +
  ggtitle("Scaled-Sentiment vs. Scaled Log Equity Price", subtitle="What we will use")
```

### 5.4.4 Regressing Sentiment to Financial Variables

The final regression model we present uses the scaled, log-transformed data with the removal of an influential outlier (observation 1 of Jan 2007).   For a reason yet to be determined, Jan 2007 generates the highest sentiment of the entire observation period.  This is arguably wrong as the Sept 2018 period was possibly the most euphoric in recent memory.  It is calculated in the code chunk below.


```{r}
mod1 = lm( z_logEquity ~ z_sentiment, data=msEQdata[2:102,])
summary(mod1)
```
The mod1 clearly has a statistically significant leading coefficient because the p-value is 6.19e-12.
The adjusted-R-squared of 37 percent suggests the model has some explanatory power.

Examining the diagnostic plots below shows:

* Q-Q plot and histogram of residuals show reasonable approximation to normality.
* residuals have relatively homogenous variance across the range of observations
* residuals have little trend in relative to the fitted values
* leverage plot has controlled for most influential outlier (observation 1)

```{r}
par(mfrow=c(3,2))
plot(mod1)
hist(mod1$residuals )
```

Finally, we present the scatterplot of regressed values overlay with the regression line to study the model fit.

```{r}
ggplot(data=msEQdata[2:102,], aes(x=z_sentiment, y=z_logEquity) ) + 
   geom_point() + 
   geom_smooth(method=lm) +
   ggtitle("ScatterPlot of Fitted Regression Model", subtitle="X=Z-Sentiment, Y=Z-LogRussell 1000 (2007-2019)")
```

### 5.4.5 Discussion of Results

There are two comments related to the time series and regression we should make.

First, the time series of sentiment clearly shows a pattern characteristic of other financial variables through the 2007-2019 period.  During the Q4 2008, at the depths of the financial crisis, sentiment appears to be at a low.
During H2 2009, when the financial markets had miraculously recovered, the sentiment spikes upward.  Other signs that sentiment is effective include the 2018 euphoria when equity markets reached daily highs during the summer and fall.
Moreover, sentiment in Q4 2018 and Q1 2019 declined in concert with the observed selloff of risk assets in the same period.

However, the sentiment index is imperfect.   The 2013 taper tantrum is not reflected correctly from a bond investor point of view.  As we recall, on May 22, 2013, bond markets panicked when Bernanke gave a speech to Congress that quantitative easing would likely be terminated at a future date.   More investigation is needed to understand the market and FOMC dynamics around that historical episode and we regard this as future work.

Second, the regressions suggests that sentiment is positively associated with equity levels.  Positive sentiment is associated with higher Russell Index 1000 levels.  We think this makes sense.  Whether sentiment causes equity markets to move or vice versa is too complex to answer with the crude econometric analysis we have conducted.  However, the trend and regression results suggest that more detailed regression analysis of sentiment difference vs. equity returns (instead of levels) both contemporaneous or lagged would promising some predictive value from sentiment analysis.   The project timeline did not allow for this more extensive regression analysis work, but we view it as fertile ground for future research.

# 6. Discussion of Results and Impact

We have discussed the results of our two analyses previously. This section focuses on broader considerations of the of FOMC statement analysis. How useful were the results in practical terms? First, the training and classification of FOMC statement attributes is clearly feasible. Our efforts combining laborious human review of the statements for labelling purposes with traditional supervised learning methods can produce useful prediction for some attributes. Other attributes such as inflation are not easily predicted using our approach. This does not mean that the problem cannot be solved with this line of attack.

Second, sentiment analysis is likely to be the more impactful tool in forecasting for investment management purposes. Our preliminary results, effectively a conventional approach to sentiment analysis, yielded a realistic looking financial sentiment indicator. We demonstrate some moderate level of explanatory power of sentiment and equity market levels using a linear regression. Much more extension regression analyses covering first differences of sentiment and price series is required. We believe the results are insufficient to be useful in practice but sufficient to justify further refinement and investigation to unlock value.

# 7. Conclusion

The project analyzed the FOMC statements using text based methods.  The results are encouraging but not definitive in their utility.  A much longer programme of research would be needed to explore the implications of this work.   Others have gone done this path.  Some companies such as JPMorgan Chase,  BlackRock have dedicated teams to analyze interest rate markets and central bank communications with machine learning tools.  One company called Prattle (www.prattle.co) has even commercialized this idea and provides central bank sentiment analysis for 16 central banks including the Fed.  Therefore, we are on the right path -- one led by earlier pioneers.

# 8. Reference

The project analyzed the FOMC statements using text based methods.  The results are encouraging but not definitive in their utility.  A much longer programme of research would be needed to explore the implications of this work.   Others have gone done this path.  Some companies such as JPMorgan Chase,  BlackRock have dedicated teams to analyze interest rate markets and central bank communications with machine learning tools.  One company called Prattle (www.prattle.co) has even commercialized this idea and provides central bank sentiment analysis for 16 central banks including the Fed.  Therefore, we are on the right path -- one led by earlier pioneers.   

# Section 8:  References

* Brusa, Francesca, Pavel Savor, and Mungo Wilson, "One Central Bank to Rule Them All", working paper, 2015, Oxford University  [https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2658953]

* Cieslak, Anna and Morse, Adair and Annette Vissing-Jorgensen, "Stock Returns Over the FOMC Cycle", Journal of Finance, forthcoming, [https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2687614]

* Cannon, San, "Sentiment of the FOMC: Unscripted", 2015, [https://www.kansascityfed.org/~/media/files/publicat/econrev/econrevarchive/2015/4q15cannon.pdf]

* Fuka, Michel and Didier Sornette "The Sentiment of the Fed", 2013, 
[https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2191937]


* Lucca, David and Francesco Trebbia, "Measuring Central Bank Communication: An Automated Approach with Application to FOMC Statements", 2011, [http://faculty.arts.ubc.ca/ftrebbi/research/lt.pdf]

