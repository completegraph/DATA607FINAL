---
title: "Text classification on the FOMC Statements"
author: "Henry Otuadinma"
date: "May 10, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(dplyr)
library(tidytext)
library(tm)
library(e1071) # new
library(R.utils)
library(DT)
library(e1071)
library(stringr)
library(lattice) # new
library(kernlab) # new
library(mlbench) # new
library(caretEnsemble) # new
library(nnet) # new
library(LiblineaR) # new
```


5.1 Text Classifications

##### The manual scores of the FOMC statements are leaveraged as a rich source of reference point for validating automated classifcations. We hope that this can serve as a good basis for future researches and there a lot of rooms for improvement. The variables classiffied are not all binary. Some are multi-classed (have more than teo classes) while only one (`Medium.Term.Rate`) is binary

##### Read the data

```{r}

fomc_data <-readRDS(file = "fomc_merged_data_v2.rds")

```


##### inspect selected variables

```{r}
head(select(fomc_data, Index,year,statement.dates,statement.length,date_mdy,Employment.Growth,Economic.Growth,Inflation,Medium.Term.Rate,Policy.Rate))

```

#### Data preparation
##### First, randomising the rows so that the statements from different eras of the economic movements can be well represented

```{r}

set.seed(1234567)

fomc_Rand <- fomc_data[sample(nrow(fomc_data)),]

```


##### Preliminary data cleansing: convert the statements' textual contents to lower and remove `the federal open market committee` and `committee` as it is present in all the statements

```{r}

customStopWords <- c("the federal open market committee", "committee")

```


```{r}
fomc_dataX <- fomc_Rand %>% mutate(statement.content = tolower(statement.content))

fomc_dataX$statement.content <- str_replace_all(fomc_dataX$statement.content, customStopWords, "")

```


##### Data Preparation: Here, we prepare the data so that it can be reused for all the classifications without the need to repeat the leaning and preparation processes gaain

```{r}

# form a corpus
corpus <- VCorpus(VectorSource(fomc_dataX$statement.content))

# Remove Punctuation
corpus <- tm_map(corpus, content_transformer(removePunctuation))

# Remove numbers
corpus <- tm_map(corpus, removeNumbers)

# Convert to lower case
corpus <- tm_map(corpus, content_transformer(tolower))

# Remove stop words
corpus <- tm_map(corpus, content_transformer(removeWords), stopwords("english"))

##Stemming
corpus <- tm_map(corpus, stemDocument)

# Remove Whitespace
corpus <- tm_map(corpus, stripWhitespace)

# Create Document Term Matrix
dtm <- DocumentTermMatrix(corpus)

# handle sparsity
corpusX <- removeSparseTerms(dtm, 0.95)

# convert to matrix
data_matrix <- as.matrix(corpusX)

```


#### Classifications

##### From here, were are going to perform classifications based on `Medium.Term.Rate`, `Employment.Growth`, `Economic.Growth`, `Inflation`, `Medium.Term.Rate`, and `Policy.Rate`   variables


5.1.1 `Medium.Term.Rate`

##### Classification targetting the `Medium.Term.Rate` variable

```{r}

mRate <- data_matrix

# attach the 'medium.term.rate' column
mRate_matrix <- cbind(mRate, fomc_dataX$Medium.Term.Rate)

# rename it to 'tone'
colnames(mRate_matrix)[ncol(mRate_matrix)] <- "tone"

# convert to data frame
mRateData <- as.data.frame(mRate_matrix)

# convert 'tone' to lower case and make it a factor column as well
mRateData$tone <- as.factor(tolower(mRateData$tone))

```

##### Partition the data into training and test sets

```{r}

mRate_n <- nrow(mRateData)
mRateTrainVolume <- round(mRate_n * 0.80)

set.seed(314)

mRateTrainIndex <- sample(mRate_n, mRateTrainVolume)
mRateTrain <- mRateData[mRateTrainIndex,]
mRateTest <- mRateData[-mRateTrainIndex,]

```


```{r}

mRateModel <- train(tone ~., data = mRateTrain, method = 'svmLinear3')

```


```{r}

mRateResult <- predict(mRateModel, newdata = mRateTest)

```


```{r}
( mRateStats = confusionMatrix( mRateResult, mRateTest$tone))
```


5.1.2 `Economic.Growth`

##### Classification targeting the `Economic.Growth` variable

```{r}

econGrowth <- data_matrix

# attach the 'Economic.Growth' column
econG_matrix <- cbind(econGrowth, tolower(fomc_dataX$Economic.Growth))

# rename it to 'growth'
colnames(econG_matrix)[ncol(econG_matrix)] <- "egrowth"

# convert to data frame
econData <- as.data.frame(econG_matrix)

# convert 'growth' to a factor column as well
econData$egrowth <- as.factor(econData$egrowth)

```

##### Partition the data into training and test sets: note that the ratios here are different from the other models

```{r}

econ_n <- nrow(econData)
econTrainVolume <- round(econ_n * 0.58)

set.seed(314)

econTrainIndex <- sample(econ_n, econTrainVolume)
econTrain <- econData[econTrainIndex,]
econTest <- econData[-econTrainIndex,]

```


```{r}

econModel <- train(egrowth ~., data = econTrain, method = 'svmLinear3')

```


```{r}

econResult <- predict(econModel, newdata = econTest)

```


```{r}
(econStats = confusionMatrix( econResult, econTest$egrowth))
```


### The `Inflation` variable


```{r}

# Create Document Term Matrix
dtmI <- DocumentTermMatrix(corpus)

# handle sparsity
corpusI <- removeSparseTerms(dtm, 0.94)

# convert to matrix
data_matrixI <- as.matrix(corpusI)


inflation <- data_matrixI

# attach the 'Inflation' column
inflation_matrix <- cbind(inflation, tolower(fomc_dataX$Inflation))

# rename it to 'inflation'
colnames(inflation_matrix)[ncol(inflation_matrix)] <- "inflation"

# convert to data frame
inflationData <- as.data.frame(inflation_matrix)

# convert 'inflation' to a factor column
inflationData$inflation <- as.factor(inflationData$inflation)

```


##### remove columns that will not contribute meaninfully to the model fitting

```{r}

infDataX <- inflationData[, -which(names(inflationData) %in% c("although", "william", "richard", "raphael", "randal", "san", "sarah","sandra", "togeth", "timothi","committe","dudley","esther"))]
 
```



```{r}

inf_n <- nrow(infDataX)
infTrainVolume <- round(inf_n * 0.70)

set.seed(314)

infTrainIndex <- sample(inf_n, infTrainVolume)
infTrain <- infDataX[infTrainIndex,]
infTest <- infDataX[-infTrainIndex,]

```


```{r}

inflationModel <- train(inflation ~., data = infTrain, method="svmLinear3")

```


```{r}

inflationResult <- predict(inflationModel, newdata = infTest)

```


```{r}
( infStats = confusionMatrix( inflationResult, infTest$inflation))
```


### The `Employment.Growth` variable

```{r}

empGrowth <- data_matrix

# attach the 'Employment.Growth column
emp_matrix <- cbind(empGrowth, tolower(fomc_dataX$Employment.Growth))

# rename it to 'empGrowth'
colnames(emp_matrix)[ncol(emp_matrix)] <- "empGrowth"

# convert to data frame
empData <- as.data.frame(emp_matrix)

# convert 'empGrowth' to a factor column as well
empData$empGrowth <- as.factor(empData$empGrowth)

```


```{r}

emp_n <- nrow(empData)
empTrainVolume <- round(emp_n * 0.80)

set.seed(314)

empTrainIndex <- sample(emp_n, empTrainVolume)
empTrain <- empData[empTrainIndex,]
empTest <- empData[-empTrainIndex,]

```


```{r}

empModel <- train(empGrowth ~., data = empTrain, method = 'svmLinear3')

```


```{r}

empResult <- predict(empModel, newdata = empTest)

```


```{r}
( empStats = confusionMatrix( empResult, empTest$empGrowth))
```



### Policy.Rate

```{r}

plRate <- data_matrix

# attach the 'Policy.Rate' column
pl_matrix <- cbind(plRate, tolower(fomc_dataX$Policy.Rate))

# rename it to 'empGrowth'
colnames(pl_matrix)[ncol(pl_matrix)] <- "policy"

# convert to data frame
plData <- as.data.frame(pl_matrix)

# convert 'policy' to a factor column as well
plData$policy <- as.factor(plData$policy)

```



```{r}

pl_n <- nrow(plData)

plTrainVolume <- round(pl_n * 0.80)

set.seed(314)

plTrainIndex <- sample(pl_n, plTrainVolume)
plTrain <- plData[empTrainIndex,]
plTest <- plData[-empTrainIndex,]

```


```{r}

plModel <- train(policy ~., data = plTrain, method = 'svmLinear3')

```


```{r}

plResult <- predict(plModel, newdata = plTest)

```


```{r}
( plStats = confusionMatrix( plResult, plTest$policy))
```


## Summary and Conclusion

Table of summary

```{r}

results <- tibble(variable = c("Medium.Term.Rate","Employment.Growth","Economic.Growth","Inflation","Policy.Rate"), modelling = c("80 : 20", "80 : 20", "58 : 42", "70 : 30", "80 : 20"), accuracy= c(100, 90, 79.1, 64.52, 95)) 

names(results)[names(results) == "accuracy"] <- "accuracy(%)"
names(results)[names(results) == "modelling"] <- "modelling(Train : Test)"

results
```

For some of the variables, extra fine-tuning of the data was not needed to achieve appreciable accuracy. But for the `Economic.Growth` we needed to adjust the ratio of training set to test data set to 58:42 to achieve an accuracy of 79.10%. For the `Inflation` we had to adjust sparsity at 0.94, removed 13 unuseful columns, adjusted the ratio of training to test data sets to 70:30 before we could achieve an accuracy of 64.52%

We can conclude that these values obtained, though not completely accurate, did go a long way to depict the nature of the economy based on the variables considered within the selected years. There are much room left for improvements and further analysis but we cannot go beyond this level right now as time will not permit us









