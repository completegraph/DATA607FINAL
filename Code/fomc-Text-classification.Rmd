---
title: "Text classification on the FOMC Statements"
author: "Henry Otuadinma"
date: "May 10, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(dplyr)
library(tm)
library(e1071) # new
library(R.utils)
library(DT)
library(e1071)
library(stringr)
library(lattice) # new
library(kernlab) # new
library(mlbench) # new
library(caretEnsemble) # new
library(nnet) # new
library(LiblineaR) # new
library(kableExtra)
```


## 5.1 Text Classifications

The manual scores of the FOMC statements are leaveraged as a rich source of reference point for validating automated classifcations. We hope that this can serve as a good basis for future researches and there a lot of rooms for improvement. The variables classiffied are not all binary. Some are multi-classed (have more than two classes) while only one (`Medium.Term.Rate`) is binary.


##### Read the data

```{r}

fomc_data <-readRDS(file = "fomc_merged_data_v2.rds")

```


##### inspect selected variables

```{r}
head(select(fomc_data, Index,year,statement.dates,statement.length,date_mdy,Employment.Growth,Economic.Growth,Inflation,Medium.Term.Rate,Policy.Rate))

```

#### Data preparation
##### First, randomising the rows so that the statements from different eras of the economic movements can be well represented

```{r}

set.seed(1234567)

fomc_Rand <- fomc_data[sample(nrow(fomc_data)),]

```


##### Preliminary data cleansing: convert the statements' textual contents to lower and remove `the federal open market committee` and `committee` as it is present in all the statements

```{r}

customStopWords <- c("the federal open market committee", "committee")

```


```{r message=FALSE, warning=FALSE}

fomc_dataX <- fomc_Rand %>% mutate(statement.content = tolower(statement.content))%>%mutate(statement.content = str_replace_all(statement.content, customStopWords, ""))

```


##### Data Preparation: Here, we prepare the data such that it can be reused for all the classifications without the need to repeat the cleaning and preparation processes again

```{r}

# form a corpus
corpus <- VCorpus(VectorSource(fomc_dataX$statement.content))

# Remove Punctuation
corpus <- tm_map(corpus, content_transformer(removePunctuation))

# Remove numbers
corpus <- tm_map(corpus, removeNumbers)

# Convert to lower case
corpus <- tm_map(corpus, content_transformer(tolower))

# Remove stop words
corpus <- tm_map(corpus, content_transformer(removeWords), stopwords("english"))

##Stemming
corpus <- tm_map(corpus, stemDocument)

# Remove Whitespace
corpus <- tm_map(corpus, stripWhitespace)

# Create Document Term Matrix
dtm <- DocumentTermMatrix(corpus)

# handle sparsity
corpusX <- removeSparseTerms(dtm, 0.30)

# convert to matrix
data_matrix <- as.matrix(corpusX)

```



### 5.1.1 `Medium.Term.Rate`

##### Classification targetting the `Medium.Term.Rate` variable

```{r}

mRate <- data_matrix

# attach the 'medium.term.rate' column
mRate_matrix <- cbind(mRate, fomc_dataX$Medium.Term.Rate)

# rename it to 'tone'
colnames(mRate_matrix)[ncol(mRate_matrix)] <- "tone"

# convert to data frame
mRateData <- as.data.frame(mRate_matrix)

# convert 'tone' to lower case and make it a factor column as well
mRateData$tone <- as.factor(tolower(mRateData$tone))

```

##### Partition the data into training and test sets

```{r}

mRate_n <- nrow(mRateData)
mRateTrainVolume <- round(mRate_n * 0.68)

set.seed(314)

mRateTrainIndex <- sample(mRate_n, mRateTrainVolume)
mRateTrain <- mRateData[mRateTrainIndex,]
mRateTest <- mRateData[-mRateTrainIndex,]

```


```{r}

mRateModel <- train(tone ~., data = mRateTrain, method = 'svmLinear3')

```


```{r}

mRateResult <- predict(mRateModel, newdata = mRateTest)

```


```{r}
( mRateStats = confusionMatrix( mRateResult, mRateTest$tone))
```


### 5.1.2 `Economic.Growth`

##### Classification targeting the `Economic.Growth` variable

```{r}

econGrowth <- data_matrix

# attach the 'Economic.Growth' column
econG_matrix <- cbind(econGrowth, tolower(fomc_dataX$Economic.Growth))

# rename it to 'growth'
colnames(econG_matrix)[ncol(econG_matrix)] <- "egrowth"

# convert to data frame
econData <- as.data.frame(econG_matrix)

# convert 'growth' to a factor column as well
econData$egrowth <- as.factor(econData$egrowth)

```

##### Partition the data into training and test sets: note that the ratios here are different from the other models

```{r}

econ_n <- nrow(econData)
econTrainVolume <- round(econ_n * 0.70)

set.seed(314)

econTrainIndex <- sample(econ_n, econTrainVolume)
econTrain <- econData[econTrainIndex,]
econTest <- econData[-econTrainIndex,]

```


```{r}

econModel <- train(egrowth ~., data = econTrain, method = 'svmLinear3')

```


```{r}

econResult <- predict(econModel, newdata = econTest)

```


```{r}
(econStats = confusionMatrix( econResult, econTest$egrowth))
```


### 5.1.3 `Inflation`

##### Classification targeting the `Inflation` variable


```{r}

# Create Document Term Matrix
dtmI <- DocumentTermMatrix(corpus)

# handle sparsity
corpusI <- removeSparseTerms(dtm, 0.80)

# convert to matrix
data_matrixI <- as.matrix(corpusI)


inflation <- data_matrixI

# attach the 'Inflation' column
inflation_matrix <- cbind(inflation, tolower(fomc_dataX$Inflation))

# rename it to 'inflation'
colnames(inflation_matrix)[ncol(inflation_matrix)] <- "inflation"

# convert to data frame
inflationData <- as.data.frame(inflation_matrix)

# convert 'inflation' to a factor column
inflationData$inflation <- as.factor(inflationData$inflation)

```


##### remove columns that will not contribute meaninfully to the model fitting

```{r}

infDataX <- inflationData[, -which(names(inflationData) %in% c("although", "william", "richard", "raphael", "randal", "san", "sarah","sandra", "togeth", "timothi","committe","dudley","esther"))]
 
```



```{r}

inf_n <- nrow(infDataX)
infTrainVolume <- round(inf_n * 0.68)

set.seed(314)

infTrainIndex <- sample(inf_n, infTrainVolume)
infTrain <- infDataX[infTrainIndex,]
infTest <- infDataX[-infTrainIndex,]

```


```{r}

inflationModel <- train(inflation ~., data = infTrain, method="svmLinear3")

```


```{r}

inflationResult <- predict(inflationModel, newdata = infTest)

```


```{r}
( infStats = confusionMatrix( inflationResult, infTest$inflation))
```


### 5.1.4 `Employment.Growth`

##### Classification targeting the `Employment.Growth` variable

```{r}

empGrowth <- data_matrix

# attach the 'Employment.Growth column
emp_matrix <- cbind(empGrowth, tolower(fomc_dataX$Employment.Growth))

# rename it to 'empGrowth'
colnames(emp_matrix)[ncol(emp_matrix)] <- "empGrowth"

# convert to data frame
empData <- as.data.frame(emp_matrix)

# convert 'empGrowth' to a factor column as well
empData$empGrowth <- as.factor(empData$empGrowth)

```


```{r}

emp_n <- nrow(empData)
empTrainVolume <- round(emp_n * 0.70)

set.seed(314)

empTrainIndex <- sample(emp_n, empTrainVolume)
empTrain <- empData[empTrainIndex,]
empTest <- empData[-empTrainIndex,]

```


```{r}

empModel <- train(empGrowth ~., data = empTrain, method = 'svmLinear3')

```


```{r}

empResult <- predict(empModel, newdata = empTest)

```


```{r}
( empStats = confusionMatrix( empResult, empTest$empGrowth))
```


### 5.1.5 `Policy.Rate`

##### Classification targeting the `Policy.Rate` variable

```{r}

plRate <- data_matrix

# attach the 'Policy.Rate' column
pl_matrix <- cbind(plRate, tolower(fomc_dataX$Policy.Rate))

# rename it to 'empGrowth'
colnames(pl_matrix)[ncol(pl_matrix)] <- "policy"

# convert to data frame
plData <- as.data.frame(pl_matrix)

# convert 'policy' to a factor column as well
plData$policy <- as.factor(plData$policy)

```



```{r}

pl_n <- nrow(plData)

plTrainVolume <- round(pl_n * 0.68)

set.seed(314)

plTrainIndex <- sample(pl_n, plTrainVolume)
plTrain <- plData[empTrainIndex,]
plTest <- plData[-empTrainIndex,]

```


```{r}

plModel <- train(policy ~., data = plTrain, method = 'svmLinear3')

```


```{r}

plResult <- predict(plModel, newdata = plTest)

```



```{r}
( plStats = confusionMatrix( plResult, plTest$policy))
```


### 5.1.6 Summary and Conclusion

Table of summary

```{r}

results <- tibble(variable = c("Medium.Term.Rate","Employment.Growth","Economic.Growth","Inflation","Policy.Rate"), modelling = c("68 : 32", "70 : 30", "70 : 30", "68 : 32", "68 : 32"), accuracy= c(93.75, 80, 66.67, 68.75, 83.33)) 

```


```{r fig.width = 10, fig.asp = .80}

 kable(results,
      col.names = linebreak(c("Variable", "Modelling (Train : Test)", "Accuracy (%)"), align = "c")) %>%
  kable_styling("striped", full_width = T) %>%
  column_spec(1:3, bold = T, color = "#000") %>%
  row_spec(1:5, bold = T, color = "#000")

```


For some of the variables, extra fine-tuning of the data was not needed to achieve appreciable accuracy. But for the `Economic.Growth` variable, we needed to adjust the ratio of training set to test data set to `58:42` to achieve an accuracy of `79.10%`. For the `Inflation` variable we had to adjust sparsity to `0.94`, removed 13 unuseful columns, adjusted the ratio of training to test data sets to `70:30` before we could achieve an accuracy of `64.52%`

We can conclude that these values obtained, though not perfect, did go a long way to align with the human based scoring/classification of the economy trends based on the variables considered within the selected years. There are much room left for improvements and further analysis but we cannot go beyond this level right now as time will not permit us


### 5.2 Reflection on Text Classification

Text classification of FOMC statements is not generally a research objective but we think it is worthwhile.   Classification addresses a potential need:  Can a machine correctly infer the opinion or direction of forward guidance or policy decisions in a structured text by the FOMC?   In this regard, the classification problem for the FOMC is isomorphic to the Ham-Spam classification of incoming emails by an email program.  However, the reader may object that FOMC statements are not so voluminous to require automated processing.   Our response is that FOMC statements are merely the first baby step in a much larger classification problem:  the public communications of all FOMC and Federal Reserve system members.   As previously explained, the FOMC members give speeches, publish articles, appear on TV interviews.   Moreover, FOMC meeting minutes are released several weeks after the policy statement is released.  These are much longer and required more effort to read and digest.  Also, the FOMC transcripts released several years after the meeting may run to over 100 pages each.  They contain word for word replay of the entire meeting (excluding private discussions).   Lastly, there are at least 16 relevant central banks around the world of interest.  Although the Fed is the world's more important central bank, the ECB, Bank of England, Bank of Japan, Bank of China, Bank of Australia, Bank of New Zealand, all produce communications.  In summary, no single person can read all central bank communications.   The ability to extract key messages from plain texts remains a valuable capability.

Our machine learning prediction backtest suggests that automated classification is feasible to detect limited features of a central bank communication.   Our algorithm succeeds at detecting medium term rate outlooks, employment growth and policy rate changes.  At these tasks, we have attained accuracy rates between 90-100 percent.   The most challenging attribute to understand is inflation (64.5%).  This is consistent with financial practitioner opinion.  Inflation is the most complex of these areas to quantify, control and manage.  That is because inflation has 4 distinct aspects:  realized inflation (price changes from past surveys), market based real yields of TIPS bonds and inflation swaps, and long term expectations of inflation, inflation measured with or without volatile sectors: food and energy.   Because the statements may treat some or all of these aspects we believe accuracy in understanding FOMC inflation views is hard.







