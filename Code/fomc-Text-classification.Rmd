---
title: "Text classification on the FOMC Statements"
author: "Henry Otuadinma"
date: "May 10, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(dplyr)
library(tidytext)
library(tm)
library(e1071)
library(R.utils)
library(DT)
library(e1071)
library(stringr)
library(lattice)
library(kernlab)
library(mlbench)
library(caretEnsemble)
#library(klaR)
library(nnet)
library(LiblineaR)
```


```{r}

fomc_data <-readRDS(file = "fomc_merged_data_v2.rds")

```


#### First, randomising the rows so that the statements from different eras of the economic movements can be well represented

```{r}

set.seed(1234567)

fomc_Rand <- fomc_data[sample(nrow(fomc_data)),]

```


```{r}

head(select(fomc_Rand, Index, statement.dates, date_mdy))
```


##### convert the statements' textual contents to lower and remove `the federal open market committee` as it is present in all the statements

```{r}

customStopWords <- c("the federal open market committee", "committee")

```


```{r}
fomc_dataX <- fomc_Rand %>% mutate(statement.content = tolower(statement.content))

fomc_dataX$statement.content <- str_replace_all(fomc_dataX$statement.content, customStopWords, "")

```


#### Classification targetting the `Medium.Term.Rate` variable

```{r}

mdtRate <- fomc_dataX %>% select(Index, statement.content, statement.dates, date_mdy, Medium.Term.Rate)

```


```{r}

mRateTrainObs <- sample(nrow(mdtRate), .80 * nrow(mdtRate), replace = FALSE)
mRateTestObs <- sample(nrow(mdtRate), .20 * nrow(mdtRate), replace = FALSE)
mRateTrain_dat <- mdtRate[mRateTrainObs,]
mRateTest_dat <- mdtRate[mRateTestObs,]

```



```{r}
train_corpus <- VCorpus(VectorSource(mRateTrain_dat$Medium.Term.Rate))
##Removing Punctuation
train_corpus <- tm_map(train_corpus, content_transformer(removePunctuation))
##Removing numbers
train_corpus <- tm_map(train_corpus, removeNumbers)
##Converting to lower case
train_corpus <- tm_map(train_corpus, content_transformer(tolower))
##Removing stop words
train_corpus <- tm_map(train_corpus, content_transformer(removeWords), stopwords("english"))
##Stemming
train_corpus <- tm_map(train_corpus, stemDocument)
##Whitespace
train_corpus <- tm_map(train_corpus, stripWhitespace)
# Create Document Term Matrix
dtm_train <- DocumentTermMatrix(train_corpus)
train_corpus <- removeSparseTerms(dtm_train, 0.85)
dtm_train_matrix <- as.matrix(train_corpus)
dtm_train_matrix <- cbind(dtm_train_matrix, mRateTrain_dat$Medium.Term.Rate)
colnames(dtm_train_matrix)[ncol(dtm_train_matrix)] <- "tone"
training_set <- as.data.frame(dtm_train_matrix)
#training_set$tone <- as.factor(tolower(training_set$tone))

```


```{r}

mdtRate_model <- train(tone ~., data = training_set, method = 'svmLinear3')

```


```{r}
#Preparing our test data. It's the same repetitive procedure.
test_corpus <- VCorpus(VectorSource(mRateTest_dat$Medium.Term.Rate))
##Removing Punctuation
test_corpus <- tm_map(test_corpus, content_transformer(removePunctuation))
##Removing numbers
test_corpus <- tm_map(test_corpus, removeNumbers)
##Converting to lower case
test_corpus <- tm_map(test_corpus, content_transformer(tolower))
##Removing stop words
test_corpus <- tm_map(test_corpus, content_transformer(removeWords), stopwords("english"))
##Stemming
test_corpus <- tm_map(test_corpus, stemDocument)
##Whitespace
test_corpus <- tm_map(test_corpus, stripWhitespace)
# Create Document Term Matrix
dtm_test <- DocumentTermMatrix(test_corpus)
test_corpus <- removeSparseTerms(dtm_test, 0.85)
dtm_test_matrix <- as.matrix(test_corpus)

```


```{r}
cat(str(training_set$tone))

```

```{r}

mrate_model_result <- predict(mdtRate_model, newdata = dtm_test_matrix)

```





















